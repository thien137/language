{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d612e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch import cuda, device, load, save, Tensor, tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Literal, NamedTuple, Callable, Tuple\n",
    "from torch.backends import mps\n",
    "from torch.optim import Adam, Optimizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea117cbc",
   "metadata": {},
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "defea63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_MODEL_STORAGE_PATH = Path(\".models\")\n",
    "\n",
    "class ModelCheckpoint(NamedTuple):\n",
    "    \"Model checkpoint data.\"\n",
    "    epoch: int\n",
    "    train_loss: float\n",
    "    val_loss: float\n",
    "    state_dict: Dict[str, Any]\n",
    "\n",
    "def count_params(model: nn.Module) -> int:\n",
    "    \"\"\"Count the number of model parameters.\"\"\"\n",
    "    return sum(len(p) for p in model.parameters())\n",
    "\n",
    "def get_best_device(\n",
    "        cuda_priority: Literal[1, 2, 3] = 1,\n",
    "        mps_priority: Literal[1, 2, 3] = 2,\n",
    "        cpu_priority: Literal[1, 2, 3] = 3,\n",
    "    ) -> device:\n",
    "    \"\"\"Return the best device available on the machine.\"\"\"\n",
    "    device_priorities = sorted(\n",
    "        ((\"cuda\", cuda_priority), (\"mps\", mps_priority), (\"cpu\", cpu_priority)),\n",
    "        key=lambda e: e[1]\n",
    "    )\n",
    "    for device_type, _ in device_priorities:\n",
    "        if device_type == \"cuda\" and cuda.is_available():\n",
    "            return device(\"cuda\")\n",
    "        elif device_type == \"mps\" and mps.is_available():\n",
    "            return device(\"mps\")\n",
    "        elif device_type == \"cpu\":\n",
    "            return device(\"cpu\")\n",
    "\n",
    "def save_model(model: nn.Module, name: str, loss: float) -> None:\n",
    "    \"\"\"Save models to disk.\"\"\"\n",
    "    if not TORCH_MODEL_STORAGE_PATH.exists():\n",
    "        TORCH_MODEL_STORAGE_PATH.mkdir()\n",
    "    model_dir = TORCH_MODEL_STORAGE_PATH / name\n",
    "    if not model_dir.exists():\n",
    "        model_dir.mkdir()\n",
    "    timestamp = datetime.now().isoformat(timespec=\"seconds\")\n",
    "    loss_str = f\"{loss:.4f}\".replace(\".\", \"_\") if loss else \"\"\n",
    "    filename = f\"trained@{timestamp};loss={loss_str}.pt\"\n",
    "    model.to(device(\"cpu\"))\n",
    "    save(model, model_dir / filename)\n",
    "\n",
    "def load_model(name: str, latest: bool = False) -> nn.Module:\n",
    "    \"\"\"Load model with best loss.\"\"\"\n",
    "    if not TORCH_MODEL_STORAGE_PATH.exists():\n",
    "        TORCH_MODEL_STORAGE_PATH.mkdir()\n",
    "    model_dir = TORCH_MODEL_STORAGE_PATH / name\n",
    "\n",
    "    if not latest:\n",
    "        stored_models = [\n",
    "            (file_path, str(file_path).split(\"loss=\")[1])\n",
    "            for file_path in model_dir.glob(\"*.pt\")\n",
    "        ]\n",
    "        model = sorted(stored_models, key=lambda e: e[1])[0][0]\n",
    "    else:\n",
    "        stored_models = [\n",
    "            (file_path, str(file_path).split(\"trained@\")[1][:19])\n",
    "            for file_path in model_dir.glob(\"*.pt\")\n",
    "        ]\n",
    "        model = sorted(stored_models, key=lambda e: datetime.fromisoformat(e[1]))[-1][0]\n",
    "\n",
    "    print(f\"loading {model}\")\n",
    "    model = load(model)\n",
    "    return model\n",
    "\n",
    "def _early_stop(train_loss: Dict[int, float], epoch_window: int = 3) -> bool:\n",
    "    \"\"\"Flag when training no longer improves loss.\"\"\"\n",
    "    if len(train_loss) < epoch_window + 1:\n",
    "        return False\n",
    "    else:\n",
    "        losses = list(train_loss.values())\n",
    "        current_loss = losses[-1]\n",
    "        avg_window_loss = sum(losses[-(epoch_window + 1) : -1]) / epoch_window\n",
    "        if current_loss >= avg_window_loss:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb21569",
   "metadata": {},
   "source": [
    "# RNN Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "afd2e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordPredictionRNN(nn.Module):\n",
    "    \"\"\"LSTM for predicting the next token in a sequence.\"\"\"\n",
    "    \n",
    "    def __init__(self, size_vocab: int, size_embed: int, size_hidden: int):\n",
    "        super().__init__()\n",
    "        self._size_hidden = size_hidden\n",
    "        self._embedding = nn.Embedding(size_vocab, size_embed)\n",
    "        self._lstm = nn.LSTM(size_embed, size_hidden, batch_first=True)\n",
    "        self._linear = nn.Linear(size_hidden, size_vocab)\n",
    "    \n",
    "    def forward(self, x: Tensor, hidden: Tensor, cell: Tensor) -> Tensor:\n",
    "        # forward step to generate words\n",
    "        out = self._embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self._lstm(out, (hidden, cell))\n",
    "        out = self._linear(out).reshape(out.shape[0], -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def initialize(self, batch_size: int, device_: device) -> Tuple[Tensor, Tensor]:\n",
    "        hidden = torch.zeros(1, batch_size, self._size_hidden, device=device_)\n",
    "        cell = torch.zeros(1, batch_size, self._size_hidden, device=device_)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "aaa69e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_step(\n",
    "    x_batch: Tensor,\n",
    "    y_batch: Tensor,\n",
    "    model: nn.Module,\n",
    "    loss_fn: Callable[[Tensor, Tensor], Tensor],\n",
    "    optimizer: Optimizer,\n",
    "    device: device,\n",
    ") -> Tensor:\n",
    "    \"\"\"One iteration of the training loop (for one batch).\"\"\"\n",
    "    model.train()\n",
    "    batch_size, sequence_length = x_batch.shape \n",
    "    \n",
    "    loss_batch = tensor(0.0, device=device)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    hidden, cell = model.initialize(batch_size, device)\n",
    "    for n in range(sequence_length):\n",
    "        y_pred, hidden, cell = model(x_batch[:, n], hidden, cell)\n",
    "        loss_batch += loss_fn(y_pred, y_batch[:, n])\n",
    "    loss_batch.backward()\n",
    "    optimizer.step() \n",
    "\n",
    "    return loss_batch / sequence_length\n",
    "\n",
    "@torch.no_grad()\n",
    "def _val_step(\n",
    "    x_batch: Tensor,\n",
    "    y_batch: Tensor,\n",
    "    model: nn.Module,\n",
    "    loss_fn: Callable[[Tensor, Tensor], Tensor],\n",
    "    device: device,\n",
    ") -> Tensor:\n",
    "    \"\"\"One iteration of the validation loop (for one batch).\"\"\"\n",
    "    model.eval()\n",
    "    batch_size, sequence_length = x_batch.shape \n",
    "    \n",
    "    loss_batch = tensor(0.0, device=device)\n",
    "\n",
    "    hidden, cell = model.initialize(batch_size, device)\n",
    "    for n in range(sequence_length):\n",
    "        y_pred, hidden, cell = model(x_batch[:, n], hidden, cell)\n",
    "        loss_batch += loss_fn(y_pred, y_batch[:, n])\n",
    "    \n",
    "    return loss_batch / sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "47b3e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_data: DataLoader,\n",
    "    val_data: DataLoader,\n",
    "    n_epochs: int,\n",
    "    learning_rate: float = 0.001,\n",
    "    random_seed: int =42,\n",
    "    device: device = get_best_device(),\n",
    ") -> Tuple[Dict[int, float], Dict[int, float], ModelCheckpoint]:\n",
    "    \"\"\"Training loop for LSTM flavoured RNNs on sequence data.\"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses: Dict[int, float] = {}\n",
    "    val_losses: Dict[int, float] = {}\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = tensor(0.0).to(device)\n",
    "        for i, (x_batch, y_batch) in enumerate((pbar := tqdm(train_data)), start=1):\n",
    "            x = x_batch.to(device, non_blocking=True)\n",
    "            y = y_batch.to(device, non_blocking=True)\n",
    "            loss_train += _train_step(x, y, model, loss_fn, optimizer, device)\n",
    "            pbar.set_description(f\"epoch {epoch} training loss = {loss_train/i:.4f}\")\n",
    "        \n",
    "        loss_val = tensor(0.0).to(device)\n",
    "        for x_batch, y_batch in val_data:\n",
    "            x = x_batch.to(device, non_blocking=True)\n",
    "            y = y_batch.to(device, non_blocking=True)\n",
    "            loss_val += _val_step(x, y, model, loss_fn, device)\n",
    "        \n",
    "        epoch_train_loss = loss_train.item() / len(train_data)\n",
    "        epoch_val_loss = loss_val.item() / len(val_data)\n",
    "        \n",
    "        if epoch == 1 or epoch_val_loss < min(val_losses.values()):\n",
    "            best_checkpoint = ModelCheckpoint(\n",
    "                epoch, epoch_train_loss, epoch_val_loss, model.state_dict().copy()\n",
    "            )\n",
    "        train_losses[epoch] = epoch_train_loss \n",
    "        val_losses[epoch] = epoch_val_loss \n",
    "\n",
    "        if _early_stop(val_losses):\n",
    "            break \n",
    "    \n",
    "    print(\"\\nbest model:\")\n",
    "    print(f\"|-- epoch: {best_checkpoint.epoch}\")\n",
    "    print(f\"|-- validation loss: {best_checkpoint.val_loss:.4f}\")\n",
    "\n",
    "    model.load_state_dict(best_checkpoint.state_dict)\n",
    "    return train_losses, val_losses, best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "265e3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: NextWordPredictionRNN,\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    strategy: Literal[\"greedy\", \"sample\", \"topk\"] = \"greedy\",\n",
    "    output_length: int = 60,\n",
    "    temperature: float = 1.0,\n",
    "    random_seed: int = 42,\n",
    "    device: device = get_best_device(),\n",
    "    *,\n",
    "    k: int = 2,\n",
    ") -> str:\n",
    "    \"\"\"Generate new text conditional on a text prompt.\"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    encoded_prompt = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    prompt_tokens = encoded_prompt[\"input_ids\"].to(device).squeeze(0).tolist()\n",
    "\n",
    "    hidden, cell = model.initialize(batch_size=1, device_=device)\n",
    "\n",
    "    # Prime model\n",
    "    for token_id in prompt_tokens[:-1]:\n",
    "        x = torch.tensor([token_id], device=device)\n",
    "        _, hidden, cell = model(x, hidden, cell)\n",
    "\n",
    "    token_sequence = prompt_tokens.copy()\n",
    "\n",
    "    for _ in range(output_length):\n",
    "        x = torch.tensor([token_sequence[-1]], device=device)\n",
    "        logits, hidden, cell = model(x, hidden, cell)\n",
    "        token_logits = logits[0] / max(temperature, 1e-6)\n",
    "\n",
    "        if strategy == \"greedy\":\n",
    "            token_pred = torch.argmax(token_logits)\n",
    "        elif strategy in {\"sample\", \"topk\"}:\n",
    "            if strategy == \"topk\":\n",
    "                v, i = torch.topk(token_logits, k)\n",
    "                filtered_logits = token_logits.clone()\n",
    "                filtered_logits[filtered_logits < v[-1]] = -float(\"Inf\")\n",
    "            else:\n",
    "                filtered_logits = token_logits\n",
    "            probs = torch.softmax(filtered_logits, dim=-1)\n",
    "            token_pred = torch.multinomial(probs, num_samples=1).item()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown decoding strategy: {strategy}\")\n",
    "\n",
    "        token_sequence.append(token_pred if isinstance(token_pred, int) else token_pred.item())\n",
    "\n",
    "    return tokenizer.decode(token_sequence, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "728c7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "ds = load_dataset(\"sedthh/gutenberg_english\", split=\"train\").select(range(20))\n",
    "\n",
    "# Load your tokenizer (still needed for accurate boundary detection)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "block_size = 256\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"TEXT\"], return_special_tokens_mask=False)\n",
    "\n",
    "tokenized = ds.map(tokenize_function, batched=True, remove_columns=[\"TEXT\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated = sum(examples[\"input_ids\"], [])\n",
    "    total_length = len(concatenated)\n",
    "    total_length = (total_length // block_size) * block_size  # drop remainder\n",
    "    # Split by chunks of block_size\n",
    "    result = {\n",
    "        \"input_ids\": [concatenated[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "    }\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized.map(group_texts, batched=True, batch_size=1, remove_columns=tokenized.column_names)\n",
    "\n",
    "split_dataset = lm_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Each batch element is {\"input_ids\": [int, int, ...]}\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in batch], dtype=torch.long)\n",
    "    x_batch = input_ids[:, :-1]\n",
    "    y_batch = input_ids[:, 1:]\n",
    "    return x_batch, y_batch\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5e2f5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_EMBED = 256\n",
    "SIZE_HIDDEN = 512\n",
    "\n",
    "MAX_EPOCHS = 5\n",
    "BATCH_SIZE = 256\n",
    "MAX_SEQ_LEN = 100\n",
    "MIN_SEQ_LEN = 10\n",
    "MIN_WORD_FREQ = 2\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "model = NextWordPredictionRNN(\n",
    "    tokenizer.vocab_size,\n",
    "    SIZE_EMBED,\n",
    "    SIZE_HIDDEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57927046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/650 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 training loss = 4.7395: 100%|██████████| 650/650 [04:20<00:00,  2.50it/s]\n",
      "epoch 2 training loss = 3.8975: 100%|██████████| 650/650 [04:17<00:00,  2.52it/s]\n",
      "epoch 3 training loss = 3.5853: 100%|██████████| 650/650 [04:18<00:00,  2.52it/s]\n",
      "epoch 4 training loss = 3.3733: 100%|██████████| 650/650 [04:18<00:00,  2.52it/s]\n",
      "epoch 5 training loss = 3.2074: 100%|██████████| 650/650 [04:18<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best model:\n",
      "|-- epoch: 3\n",
      "|-- validation loss: 3.9789\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, best_checkpoint = train(\n",
    "    model=model,\n",
    "    train_data=train_dataloader,\n",
    "    val_data=val_dataloader,\n",
    "    n_epochs=MAX_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fd7b7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is the worst timeline given the circumstances. You must have a little remarkable, and that they are not hers, but they are not really friendly to the other. ” “ I am not able to do, ” he cried. “ I don ’ t know what you mean to do it, ” said Alice, “ if you ’ d have been,'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model=model,\n",
    "    prompt=\"this is the worst timeline given the circumstances. You must\",\n",
    "    tokenizer=tokenizer,\n",
    "    strategy=\"topk\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab145c",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fe888cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module: nn.Module, N: int) -> nn.ModuleList:\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "054e3f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8304a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab: int):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "db340712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c8911970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"Layernorm and Residual connection in between layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: Tensor, sublayer: nn.Module):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "09bab405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask=None, dropout=None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores.masked_fill(mask == 0, 1e-9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a4de9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Computes multi head attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, heads: int, d_model: int, dropout: float = 0.1):\n",
    "        \"Take in model size and number of heads\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads \n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        nbatches = q.size(0)\n",
    "        \n",
    "        # 1) Do linear projections to Q, K, V\n",
    "        q, k, v = [\n",
    "            lin(x).view(nbatches, -1, self.heads, self.d_k).transpose(1, 2) \n",
    "            for lin, x in zip(self.linears, (q, k, v))\n",
    "        ]\n",
    "        \n",
    "        # 2) Run attention on Q, K, V\n",
    "        a = attention(q, k, v, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "        # 3) Concatenate heads \n",
    "        a = (\n",
    "            a.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.heads * self.d_k)\n",
    "        )\n",
    "        del q\n",
    "        del k \n",
    "        del v \n",
    "        return self.linears[-1](a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e640837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Decoder is made of masked self-attention and feed forward\"\"\"\n",
    "    def __init__(self, d_model: int, self_attn: MultiHeadAttention, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.d_model = d_model \n",
    "        self.self_attn = self_attn \n",
    "        self.feed_forward = feed_forward \n",
    "        self.sublayer = clones(SublayerConnection(d_model, dropout), 2)\n",
    "    \n",
    "    def forward(self, x: Tensor, mask: Tensor):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ac4e2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer: DecoderLayer, N: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(layer.d_model)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor, \n",
    "        mask: Tensor\n",
    "    ):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "547000ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step. Used after decoder. So use NLLLoss() instead of CrossEntropyLoss()\"\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor\n",
    "    ):\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "be00de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT for language generation.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size: int, \n",
    "        N: int = 6, \n",
    "        d_model: int = 512,\n",
    "        d_ff: int = 2048,\n",
    "        heads: int = 8,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super(GPT, self).__init__()\n",
    "        attn = MultiHeadAttention(heads, d_model)\n",
    "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        position = PositionalEncoding(d_model, dropout)\n",
    "        self.embed = nn.Sequential(Embeddings(d_model, vocab_size), position)\n",
    "        self.decoder = Decoder(DecoderLayer(d_model, attn, ff, dropout), N)\n",
    "        self.generator = Generator(d_model, vocab_size)\n",
    "    \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        return self.generator(self.decoder(self.embed(x), mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fc3934c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size: int):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, 1, size, size) # (1, sequence_length, sequence_length)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return (subsequent_mask == 0).to(float) # (1, sequence_length, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c36b15ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Untrained Model Prediction: tensor([[ 0, 10,  4,  6, 10,  4,  6, 10,  4,  6]])\n",
      "Example Untrained Model Prediction: tensor([[ 0,  2, 10, 10, 10, 10, 10, 10, 10, 10]])\n",
      "Example Untrained Model Prediction: tensor([[0, 7, 7, 8, 8, 9, 4, 9, 4, 2]])\n",
      "Example Untrained Model Prediction: tensor([[ 0, 10,  0, 10,  1,  8, 10,  1,  8, 10]])\n",
      "Example Untrained Model Prediction: tensor([[0, 3, 3, 3, 3, 3, 3, 3, 1, 8]])\n",
      "Example Untrained Model Prediction: tensor([[0, 8, 8, 8, 8, 8, 8, 8, 9, 2]])\n",
      "Example Untrained Model Prediction: tensor([[0, 2, 3, 2, 1, 8, 0, 2, 1, 8]])\n",
      "Example Untrained Model Prediction: tensor([[ 0, 10,  1,  1,  1,  1, 10,  1, 10,  1]])\n",
      "Example Untrained Model Prediction: tensor([[0, 8, 8, 8, 8, 8, 8, 8, 8, 8]])\n",
      "Example Untrained Model Prediction: tensor([[0, 0, 0, 5, 4, 4, 5, 8, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "def inference_test():\n",
    "    model = GPT(11, 2)\n",
    "    model.eval() \n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "\n",
    "    ys = torch.zeros(1, 1).type_as(src)\n",
    "    for _ in range(9):\n",
    "        out = nn.functional.log_softmax(model(ys, subsequent_mask(ys.size(1))), dim=-1)\n",
    "        prob = torch.argmax(out[:,-1]).item()\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1, 1).type_as(src).fill_(prob)], dim=1\n",
    "        )\n",
    "    \n",
    "    print(\"Example Untrained Model Prediction:\", ys)\n",
    "\n",
    "def run_tests():\n",
    "    for _ in range(10):\n",
    "        inference_test()\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9defe3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_step(\n",
    "    x_batch: Tensor,\n",
    "    y_batch: Tensor,\n",
    "    model: nn.Module,\n",
    "    loss_fn: Callable[[Tensor, Tensor], Tensor],\n",
    "    optimizer: Optimizer,\n",
    "    device: device,\n",
    ") -> Tensor:\n",
    "    \"\"\"One iteration of the training loop (for one batch).\"\"\"\n",
    "    model.train()\n",
    "    _, sequence_length = x_batch.shape \n",
    "    \n",
    "    loss_batch = tensor(0.0, device=device)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    y_pred = model(x_batch, subsequent_mask(sequence_length))\n",
    "    loss_batch += loss_fn(y_pred.permute(0, 2, 1), y_batch)\n",
    "    \n",
    "    loss_batch.backward()\n",
    "    optimizer.step() \n",
    "\n",
    "    return loss_batch / sequence_length\n",
    "\n",
    "@torch.no_grad()\n",
    "def _val_step(\n",
    "    x_batch: Tensor,\n",
    "    y_batch: Tensor,\n",
    "    model: nn.Module,\n",
    "    loss_fn: Callable[[Tensor, Tensor], Tensor],\n",
    "    device: device,\n",
    ") -> Tensor:\n",
    "    \"\"\"One iteration of the validation loop (for one batch).\"\"\"\n",
    "    model.eval()\n",
    "    batch_size, sequence_length = x_batch.shape \n",
    "    \n",
    "    loss_batch = tensor(0.0, device=device)\n",
    "\n",
    "    y_pred = model(x_batch, subsequent_mask(sequence_length))\n",
    "    loss_batch += loss_fn(y_pred, y_batch)\n",
    "    \n",
    "    return loss_batch / sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ff047847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_data: DataLoader,\n",
    "    val_data: DataLoader,\n",
    "    n_epochs: int,\n",
    "    learning_rate: float = 0.001,\n",
    "    random_seed: int =42,\n",
    "    device: device = \"cpu\",\n",
    ") -> Tuple[Dict[int, float], Dict[int, float], ModelCheckpoint]:\n",
    "    \"\"\"Training loop for LSTM flavoured RNNs on sequence data.\"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses: Dict[int, float] = {}\n",
    "    val_losses: Dict[int, float] = {}\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = tensor(0.0).to(device)\n",
    "        for i, (x_batch, y_batch) in enumerate((pbar := tqdm(train_data)), start=1):\n",
    "            x = x_batch.to(device, non_blocking=True)\n",
    "            y = y_batch.to(device, non_blocking=True)\n",
    "            loss_train += _train_step(x, y, model, loss_fn, optimizer, device)\n",
    "            pbar.set_description(f\"epoch {epoch} training loss = {loss_train/i:.4f}\")\n",
    "        \n",
    "        loss_val = tensor(0.0).to(device)\n",
    "        for x_batch, y_batch in val_data:\n",
    "            x = x_batch.to(device, non_blocking=True)\n",
    "            y = y_batch.to(device, non_blocking=True)\n",
    "            loss_val += _val_step(x, y, model, loss_fn, device)\n",
    "        \n",
    "        epoch_train_loss = loss_train.item() / len(train_data)\n",
    "        epoch_val_loss = loss_val.item() / len(val_data)\n",
    "        \n",
    "        if epoch == 1 or epoch_val_loss < min(val_losses.values()):\n",
    "            best_checkpoint = ModelCheckpoint(\n",
    "                epoch, epoch_train_loss, epoch_val_loss, model.state_dict().copy()\n",
    "            )\n",
    "        train_losses[epoch] = epoch_train_loss \n",
    "        val_losses[epoch] = epoch_val_loss \n",
    "\n",
    "        if _early_stop(val_losses):\n",
    "            break \n",
    "    \n",
    "    print(\"\\nbest model:\")\n",
    "    print(f\"|-- epoch: {best_checkpoint.epoch}\")\n",
    "    print(f\"|-- validation loss: {best_checkpoint.val_loss:.4f}\")\n",
    "\n",
    "    model.load_state_dict(best_checkpoint.state_dict)\n",
    "    return train_losses, val_losses, best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bfa86389",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(tokenizer.vocab_size, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "de2d41b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 training loss = 0.0273:  15%|█▌        | 98/650 [03:47<21:20,  2.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[260], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, val_losses, best_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[217], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, n_epochs, learning_rate, random_seed, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m     y \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m     loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_train\u001b[38;5;241m/\u001b[39mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m tensor(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[216], line 17\u001b[0m, in \u001b[0;36m_train_step\u001b[0;34m(x_batch, y_batch, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(x_batch, subsequent_mask(sequence_length))\n\u001b[0;32m---> 17\u001b[0m loss_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss_batch\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \n",
      "File \u001b[0;32m/opt/miniconda3/envs/lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lab/lib/python3.10/site-packages/torch/nn/modules/loss.py:1385\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lab/lib/python3.10/site-packages/torch/nn/functional.py:3458\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3457\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3459\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3462\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, best_checkpoint = train(\n",
    "    model=model,\n",
    "    train_data=train_dataloader,\n",
    "    val_data=val_dataloader,\n",
    "    n_epochs=MAX_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e3236458",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: NextWordPredictionRNN,\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    strategy: Literal[\"greedy\", \"sample\", \"topk\"] = \"greedy\",\n",
    "    output_length: int = 60,\n",
    "    temperature: float = 1.0,\n",
    "    random_seed: int = 42,\n",
    "    device: device = \"cpu\",\n",
    "    *,\n",
    "    k: int = 2,\n",
    ") -> str:\n",
    "    \"\"\"Generate new text conditional on a text prompt.\"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    encoded_prompt = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    prompt_tokens = encoded_prompt[\"input_ids\"].to(device).squeeze(0).tolist()\n",
    "\n",
    "    token_sequence = tensor(prompt_tokens.copy()).unsqueeze(0)\n",
    "\n",
    "    for _ in range(output_length):\n",
    "        out = nn.functional.log_softmax(model(token_sequence, subsequent_mask(token_sequence.size(1))), dim=-1)\n",
    "        prob = torch.argmax(out[:,-1]).item()\n",
    "        token_sequence = torch.cat(\n",
    "            [token_sequence, torch.empty(1, 1).type_as(token_sequence).fill_(prob)], dim=1\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(token_sequence.flatten(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1ef912c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is the worst timeline given the circumstances. You must and the o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model=model,\n",
    "    prompt=\"this is the worst timeline given the circumstances. You must\",\n",
    "    tokenizer=tokenizer,\n",
    "    strategy=\"topk\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a6814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
