{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d612e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda, device, load, save, Tensor, tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Literal, NamedTuple, Callable, Tuple\n",
    "from torch.backends import mps\n",
    "from torch.nn import LSTM, Embedding, Linear, Module, CrossEntropyLoss, Module\n",
    "from torch.optim import Adam, Optimizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea117cbc",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "defea63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_MODEL_STORAGE_PATH = Path(\".models\")\n",
    "\n",
    "class ModelCheckpoint(NamedTuple):\n",
    "    \"Model checkpoint data.\"\n",
    "    epoch: int\n",
    "    train_loss: float\n",
    "    val_loss: float\n",
    "    state_dict: Dict[str, Any]\n",
    "\n",
    "def count_params(model: Module) -> int:\n",
    "    \"\"\"Count the number of model parameters.\"\"\"\n",
    "    return sum(len(p) for p in model.parameters())\n",
    "\n",
    "def get_best_device(\n",
    "        cuda_priority: Literal[1, 2, 3] = 1,\n",
    "        mps_priority: Literal[1, 2, 3] = 2,\n",
    "        cpu_priority: Literal[1, 2, 3] = 3,\n",
    "    ) -> device:\n",
    "    \"\"\"Return the best device available on the machine.\"\"\"\n",
    "    device_priorities = sorted(\n",
    "        ((\"cuda\", cuda_priority), (\"mps\", mps_priority), (\"cpu\", cpu_priority)),\n",
    "        key=lambda e: e[1]\n",
    "    )\n",
    "    for device_type, _ in device_priorities:\n",
    "        if device_type == \"cuda\" and cuda.is_available():\n",
    "            return device(\"cuda\")\n",
    "        elif device_type == \"mps\" and mps.is_available():\n",
    "            return device(\"mps\")\n",
    "        elif device_type == \"cpu\":\n",
    "            return device(\"cpu\")\n",
    "\n",
    "def save_model(model: Module, name: str, loss: float) -> None:\n",
    "    \"\"\"Save models to disk.\"\"\"\n",
    "    if not TORCH_MODEL_STORAGE_PATH.exists():\n",
    "        TORCH_MODEL_STORAGE_PATH.mkdir()\n",
    "    model_dir = TORCH_MODEL_STORAGE_PATH / name\n",
    "    if not model_dir.exists():\n",
    "        model_dir.mkdir()\n",
    "    timestamp = datetime.now().isoformat(timespec=\"seconds\")\n",
    "    loss_str = f\"{loss:.4f}\".replace(\".\", \"_\") if loss else \"\"\n",
    "    filename = f\"trained@{timestamp};loss={loss_str}.pt\"\n",
    "    model.to(device(\"cpu\"))\n",
    "    save(model, model_dir / filename)\n",
    "\n",
    "def load_model(name: str, latest: bool = False) -> Module:\n",
    "    \"\"\"Load model with best loss.\"\"\"\n",
    "    if not TORCH_MODEL_STORAGE_PATH.exists():\n",
    "        TORCH_MODEL_STORAGE_PATH.mkdir()\n",
    "    model_dir = TORCH_MODEL_STORAGE_PATH / name\n",
    "\n",
    "    if not latest:\n",
    "        stored_models = [\n",
    "            (file_path, str(file_path).split(\"loss=\")[1])\n",
    "            for file_path in model_dir.glob(\"*.pt\")\n",
    "        ]\n",
    "        model = sorted(stored_models, key=lambda e: e[1])[0][0]\n",
    "    else:\n",
    "        stored_models = [\n",
    "            (file_path, str(file_path).split(\"trained@\")[1][:19])\n",
    "            for file_path in model_dir.glob(\"*.pt\")\n",
    "        ]\n",
    "        model = sorted(stored_models, key=lambda e: datetime.fromisoformat(e[1]))[-1][0]\n",
    "\n",
    "    print(f\"loading {model}\")\n",
    "    model = load(model)\n",
    "    return model\n",
    "\n",
    "def _early_stop(train_loss: Dict[int, float], epoch_window: int = 3) -> bool:\n",
    "    \"\"\"Flag when training no longer improves loss.\"\"\"\n",
    "    if len(train_loss) < epoch_window + 1:\n",
    "        return False\n",
    "    else:\n",
    "        losses = list(train_loss.values())\n",
    "        current_loss = losses[-1]\n",
    "        avg_window_loss = sum(losses[-(epoch_window + 1) : -1]) / epoch_window\n",
    "        if current_loss >= avg_window_loss:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb21569",
   "metadata": {},
   "source": [
    "# RNN Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45df9f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 10/10 [00:01<00:00,  5.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "ds = load_dataset(\"sedthh/gutenberg_english\", split=\"train\").select(range(10))\n",
    "\n",
    "# Load your tokenizer (still needed for accurate boundary detection)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "block_size = 256\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"TEXT\"], return_special_tokens_mask=False)\n",
    "\n",
    "tokenized = ds.map(tokenize_function, batched=True, remove_columns=[\"TEXT\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "728c7efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 47.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated = sum(examples[\"input_ids\"], [])\n",
    "    total_length = len(concatenated)\n",
    "    total_length = (total_length // block_size) * block_size  # drop remainder\n",
    "    # Split by chunks of block_size\n",
    "    result = {\n",
    "        \"input_ids\": [concatenated[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "    }\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized.map(group_texts, batched=True, batch_size=1, remove_columns=tokenized.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5df48398",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = lm_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a60e91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Each batch element is {\"input_ids\": [int, int, ...]}\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in batch], dtype=torch.long)\n",
    "    x_batch = input_ids[:, :-1]\n",
    "    y_batch = input_ids[:, 1:]\n",
    "    return x_batch, y_batch\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afd2e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordPredictionRNN(Module):\n",
    "    \"\"\"LSTM for predicting the next token in a sequence.\"\"\"\n",
    "    \n",
    "    def __init__(self, size_vocab: int, size_embed: int, size_hidden: int):\n",
    "        super().__init__()\n",
    "        self._size_hidden = size_hidden\n",
    "        self._embedding = Embedding(size_vocab, size_embed)\n",
    "        self._lstm = LSTM(size_embed, size_hidden, batch_first=True)\n",
    "        self._linear = Linear(size_hidden, size_vocab)\n",
    "    \n",
    "    def forward(self, x: Tensor, hidden: Tensor, cell: Tensor) -> Tensor:\n",
    "        # forward step to generate words\n",
    "        out = self._embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self._lstm(out, (hidden, cell))\n",
    "        out = self._linear(out).reshape(out.shape[0], -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def initialize(self, batch_size: int, device_: device) -> Tuple[Tensor, Tensor]:\n",
    "        hidden = torch.zeros(1, batch_size, self._size_hidden, device=device_)\n",
    "        cell = torch.zeros(1, batch_size, self._size_hidden, device=device_)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aaa69e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_step(\n",
    "    x_batch: Tensor,\n",
    "    y_batch: Tensor,\n",
    "    model: Module,\n",
    "    loss_fn: Callable[[Tensor, Tensor], Tensor],\n",
    "    optimizer: Optimizer,\n",
    "    device: device,\n",
    ") -> Tensor:\n",
    "    \"\"\"One iteration of the training loop (for one batch).\"\"\"\n",
    "    model.train()\n",
    "    batch_size, sequence_length = x_batch.shape \n",
    "    \n",
    "    loss_batch = tensor(0.0, device=device)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    hidden, cell = model.initialize(batch_size, device)\n",
    "    for n in range(sequence_length):\n",
    "        y_pred, hidden, cell = model(x_batch[:, n], hidden, cell)\n",
    "        loss_batch += loss_fn(y_pred, y_batch[:, n])\n",
    "    loss_batch.backward()\n",
    "    optimizer.step() \n",
    "\n",
    "    return loss_batch / sequence_length\n",
    "\n",
    "@torch.no_grad()\n",
    "def _val_step(\n",
    "    x_batch: Tensor,\n",
    "    y_batch: Tensor,\n",
    "    model: Module,\n",
    "    loss_fn: Callable[[Tensor, Tensor], Tensor],\n",
    "    device: device,\n",
    ") -> Tensor:\n",
    "    \"\"\"One iteration of the validation loop (for one batch).\"\"\"\n",
    "    model.eval()\n",
    "    batch_size, sequence_length = x_batch.shape \n",
    "    \n",
    "    loss_batch = tensor(0.0, device=device)\n",
    "\n",
    "    hidden, cell = model.initialize(batch_size, device)\n",
    "    for n in range(sequence_length):\n",
    "        y_pred, hidden, cell = model(x_batch[:, n], hidden, cell)\n",
    "        loss_batch += loss_fn(y_pred, y_batch[:, n])\n",
    "    \n",
    "    return loss_batch / sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47b3e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: Module,\n",
    "    train_data: DataLoader,\n",
    "    val_data: DataLoader,\n",
    "    n_epochs: int,\n",
    "    learning_rate: float = 0.001,\n",
    "    random_seed: int =42,\n",
    "    device: device = 'cpu',\n",
    ") -> Tuple[Dict[int, float], Dict[int, float], ModelCheckpoint]:\n",
    "    \"\"\"Training loop for LSTM flavoured RNNs on sequence data.\"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "    model.to(device)\n",
    "\n",
    "    global PAD_TOKEN_IDX\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = CrossEntropyLoss(ignore_index=PAD_TOKEN_IDX)\n",
    "    \n",
    "    train_losses: Dict[int, float] = {}\n",
    "    val_losses: Dict[int, float] = {}\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = tensor(0.0).to(device)\n",
    "        for i, (x_batch, y_batch) in enumerate((pbar := tqdm(train_data)), start=1):\n",
    "            x = x_batch.to(device, non_blocking=True)\n",
    "            y = y_batch.to(device, non_blocking=True)\n",
    "            loss_train += _train_step(x, y, model, loss_fn, optimizer, device)\n",
    "            pbar.set_description(f\"epoch {epoch} training loss = {loss_train/i:.4f}\")\n",
    "        \n",
    "        loss_val = tensor(0.0).to(device)\n",
    "        for x_batch, y_batch in val_data:\n",
    "            x = x_batch.to(device, non_blocking=True)\n",
    "            y = y_batch.to(device, non_blocking=True)\n",
    "            loss_val += _val_step(x, y, model, loss_fn, device)\n",
    "        \n",
    "        epoch_train_loss = loss_train.item() / len(train_data)\n",
    "        epoch_val_loss = loss_val.item() / len(val_data)\n",
    "        \n",
    "        if epoch == 1 or epoch_val_loss < min(val_losses.values()):\n",
    "            best_checkpoint = ModelCheckpoint(\n",
    "                epoch, epoch_train_loss, epoch_val_loss, model.state_dict().copy()\n",
    "            )\n",
    "        train_losses[epoch] = epoch_train_loss \n",
    "        val_losses[epoch] = epoch_val_loss \n",
    "\n",
    "        if _early_stop(val_losses):\n",
    "            break \n",
    "    \n",
    "    print(\"\\nbest model:\")\n",
    "    print(f\"|-- epoch: {best_checkpoint.epoch}\")\n",
    "    print(f\"|-- validation loss: {best_checkpoint.val_loss:.4f}\")\n",
    "\n",
    "    model.load_state_dict(best_checkpoint.state_dict)\n",
    "    return train_losses, val_losses, best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "265e3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: NextWordPredictionRNN,\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    strategy: Literal[\"greedy\", \"sample\", \"topk\"] = \"greedy\",\n",
    "    output_length: int = 60,\n",
    "    temperature: float = 1.0,\n",
    "    random_seed: int = 42,\n",
    "    device: device = \"cpu\",\n",
    "    *,\n",
    "    k: int = 2,\n",
    ") -> str:\n",
    "    \"\"\"Generate new text conditional on a text prompt.\"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    encoded_prompt = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    prompt_tokens = encoded_prompt[\"input_ids\"].to(device).squeeze(0).tolist()\n",
    "\n",
    "    hidden, cell = model.initialize(batch_size=1, device_=device)\n",
    "\n",
    "    # Prime model\n",
    "    for token_id in prompt_tokens[:-1]:\n",
    "        x = torch.tensor([token_id], device=device)\n",
    "        _, hidden, cell = model(x, hidden, cell)\n",
    "\n",
    "    token_sequence = prompt_tokens.copy()\n",
    "\n",
    "    for _ in range(output_length):\n",
    "        x = torch.tensor([token_sequence[-1]], device=device)\n",
    "        logits, hidden, cell = model(x, hidden, cell)\n",
    "        token_logits = logits[0] / max(temperature, 1e-6)\n",
    "\n",
    "        if strategy == \"greedy\":\n",
    "            token_pred = torch.argmax(token_logits)\n",
    "        elif strategy in {\"sample\", \"topk\"}:\n",
    "            if strategy == \"topk\":\n",
    "                v, i = torch.topk(token_logits, k)\n",
    "                filtered_logits = token_logits.clone()\n",
    "                filtered_logits[filtered_logits < v[-1]] = -float(\"Inf\")\n",
    "            else:\n",
    "                filtered_logits = token_logits\n",
    "            probs = torch.softmax(filtered_logits, dim=-1)\n",
    "            token_pred = torch.multinomial(probs, num_samples=1).item()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown decoding strategy: {strategy}\")\n",
    "\n",
    "        token_sequence.append(token_pred if isinstance(token_pred, int) else token_pred.item())\n",
    "\n",
    "    return tokenizer.decode(token_sequence, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e2f5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_EMBED = 256\n",
    "SIZE_HIDDEN = 512\n",
    "\n",
    "MAX_EPOCHS = 30\n",
    "BATCH_SIZE = 256\n",
    "MAX_SEQ_LEN = 100\n",
    "MIN_SEQ_LEN = 10\n",
    "MIN_WORD_FREQ = 2\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "model = NextWordPredictionRNN(\n",
    "    tokenizer.vocab_size,\n",
    "    SIZE_EMBED,\n",
    "    SIZE_HIDDEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57927046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 training loss = 3.8046:  29%|██▊       | 75/261 [05:34<15:07,  4.88s/it]"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, best_checkpoint = train(\n",
    "    model=model,\n",
    "    train_data=train_dataloader,\n",
    "    val_data=val_dataloader,\n",
    "    n_epochs=MAX_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7fd7b7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is the worst timeline given the circumstances. You must she could not, ” Alice said, Thou art a man, she could not, and said, I am the LORD ’ s house, and she could not, and she said, I am the LORD, and said, I am the LORD ’ s name ’'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model=model,\n",
    "    prompt=\"this is the worst timeline given the circumstances. You must\",\n",
    "    tokenizer=tokenizer,\n",
    "    strategy=\"topk\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e28b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
